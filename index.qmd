---
title: "Sample Report - Data Science Capstone"
author: "Dinesh"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

### What is "mehtod"?

This is an introduction to Kernel regression, which is a non-parametric
estimator that estimates the conditional expectation of two variables
which is random. The goal of a kernel regression is to discover the
non-linear relationship between two random variables. To discover the
non-linear relationship, kernel estimator or kernel smoothing is the
main method to estimate the curve for non-parametric statistics. In
kernel estimator, weight function is known as kernel function
[@efr2008]. Cite this paper [@bro2014principal]. The GEE [@wang2014].

## Method

The common non-parametric regression model is
$Y_i = m(X_i) + \varepsilon_i$, where $Y_i$ can be defined as the sum of
the regression function value $m(x)$ for $X_i$. Here $m(x)$ is unknown
and $\varepsilon_i$ some errors. With the help of this definition, we
can create the estimation for local averaging i.e. $m(x)$ can be
estimated with the product of $Y_i$ average and $X_i$ is near to $x$. In
other words, this means that we are discovering the line through the
data points with the help of surrounding data points. The estimation
formula is printed below [@R-base]:

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$ $W_n(x)$ is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if $X_i$ is far from $x$.

Another equation:

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data and Visualization

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Statistical Modeling

```{r}

```

### Conclusion

Article 1:

This study emphasizes that in an era of increased data output, there is
a growing demand for strong, rapid, and accurate algorithms for data
analysis. Benefits of Support Vector Machines (SVM) as a potent machine
learning technique for a range of statistical learning issues, including
bioinformatics, are highlighted. In particular, SVMs are well-known for
their use of kernel functions and generalization theory, which allow for
the creation of non-linear decision surfaces for tasks involving
regression and classification. The study presents a summary of Support
Vector Machines (SVM), examines both linear and non-linear
classification, and highlights the significance of kernel selection. It
also covers a variety of data mining classification approaches before
recognizing some of SVM's drawbacks, including the difficulty of
selecting the best kernel for a given situation and issues with
performance and size.

Article 2:

The Development and Application of Support Vector Machine

Jun, Z. (2021). The development and application of support vector
machine. In *Journal of Physics: Conference Series* (Vol. 1748, No. 5,
p. 052006). IOP Publishing.

The article gives an introduction to the Support Vector Machine (SVM)
method, highlighting its benefits, fundamental ideas, and uses in face
identification, protein prediction, and contemporary machining. It
explores the theoretical underpinnings of support vector machines (SVM),
going over ideas like kernel functions and hard and soft margins. The
application sections show off effective SVM implementations across a
range of disciplines, demonstrating its effectiveness in face detection,
protein type prediction, and bearing defect diagnostics. The paper
highlights the adaptability and generalization power of SVM while also
pointing out some of its possible drawbacks, especially with regard to
training efficiency for large-scale datasets. The study's finding
implies that future work might concentrate on enhancing SVM algorithms
and increasing the range of real-world uses for them.

Overview of Support Vector Machine in Modeling Machining Performances

Deris, A. M., Zain, A. M., & Sallehuddin, R. (2011). Overview of support
vector machine in modeling machining performances. *Procedia
Engineering*, *24*, 308-312.

The use of Support Vector Machines (SVM) to model machining performances
is covered in this study, focusing on both traditional and contemporary
machining procedures. Researchers from Universiti Teknologi Malaysia
draw attention to the difficulties in simulating and improving machining
operations as well as the growing use of computational methods like SVM.
Surface roughness, tool wear, tool breakage, and defect diagnostics are
just a few of the machining factors that are modeled by the Support
Vector Machine (SVM), a potent mathematical tool for data
classification, regression, and function estimation. The review
highlights how well SVM handles the intricacy of machining processes and
how widely it is used to produce high-quality items with economical
predictions. The study emphasizes SVM's function as a flexible and
trustworthy modeling approach in the developing field of machining.

## References
